% !TEX root =  ../Dissertation.tex

\chapter{Testing}

Testing ensures that the tools behave correctly, it also allows for better
maintainability as any changes can be automatically checked for issues. For
Agda Tree, unit tests have been created for each query. This gives some
guarantee that the queries are working as expected and that any future changes
still return the same results. These tests were also timed, to measure the
performance of the queries, while some will vary in performance depending on
their input it gives a good estimate of their expected performance.

For Agda Comp, the performance of the compilation is the overall goal, so the
testing is centred around the strategies. Checking that the strategies compile
the projects correctly, such that no module is left not compiled and that a
module isn't compiled at the same time as one of its dependencies. The time to
creat the compilation order with the strategy is also measured, but it is
insignificant compared to the time to compile an Agda project.

\subsection{Agda Tree Unit Tests}

Unit tests have been created for each query and the options that the queries
can take. These test use the TypeTopology definition and module dependency
graph to test the results of the queries. While testing every combination of
input for this graph and checking the output isn't possible, the test instead
test one example of the use of that query. For example, to test the
dependencies queries the definition
"InfinitePigeon.Addition.n-plus-zero-equals-n" is used, and checked if the
output is correct given a hard coded expected response. This is done for each
query, along with their different options so for the dependencies query there
is also a unit test for the indirect option. Integration testing while
possible, isn't necessary as the CLI parsers are generated from the query
functions. So the CLI and the queries don't need to be tested for their
integration as the CLI is based of the queries directly. 

\subsection{Agda Tree Performance} \label{sub:agda tree performance}

The performance of the queries has been tested, although, this will be highly
dependent of the size of the graph and the Agda project. In this case
TypeTopology is a large project with about 50,000 definitions, this gives a
good representation of how long the queries will take with a most projects.
Most queries can be completed is less than a millisecond and the rest being
completed in under 20 milliseconds. The exceptions being the cycles queries,
finding how many indirect uses a definition has and finding the path between
two definitions or a leaf. The cycles queries takes about 622 ms to be
completed and the indirect uses takes about 3804 ms to be completed. Since the
queries have recursively calls that traverse the whole graph, this is expected.
The path between two definitions query can vary depending on the definitions,
due to cycles and size of the graph the time to complete this query can go from
less than a millisecond to impossible.

Lastly, the create tree query takes the longest time to be completed. As it has
to compile the entire Agda project, then parse all the s-expression files into
a graph. For TypeTopology this can take 10 minutes or more, although, this is
mitigated as the command would only need to be run once and the graph can then
be re-used.

\subsection{Agda Comp Strategy Validation}

The testing for Agda Comp centered around the correctness and safety of the
strategies. For a compilation strategy to be valid it must be correct, that is
it compiles all the modules in the project. It must also be safe, so a module
can't be compiled at the same time as one of its dependencies or itself. These
two properties are checked for the level strategy and the disjoint level
strategy described in Sub Section \ref{sub:key}. The correctness check is done
by adding each module and its dependencies to a set, if this set is the same as
the set of all the modules in the dependency graph then it is correct as all
modules are compiled eventually. The safety check is done by checking each step
of the compilation order, if there are some index files that are compiled
together then the dependencies of those index files are checked to be disjoint
from each other. If they are disjoint, then they are removed from the graph and
moves on to the next step in the compilation order. By the end if all index
files compiled in parallel had disjoint dependencies then the overall
compilation is safe.

The performance of Agda Comp is highly dependent on the Agda project, and will
be assessed on the evaluation Section \ref{sec:key} as the purpose of the tool
is to speed up compilation. The time to create the level and level B tests is
1-millisecond and level disjoint takes about 50 milliseconds. This time is
insignificant compared to the 5 minutes it normally takes TypeTopology to be
compiled.

\subsection{Conclusion}

Agda Tree unit tests each query, checking if a certain input gives the correct
output. Making it easier to make changes while maintaining stability as the
unit tests check that the output remains the same. The unit testes are timed,
with most queries being negligible except for cycles, path between, path to
leaf and indirect uses which can take from half a second to unending depending
on the input.  

Agda Comp was tested on the strategies it used, whether the strategies were
correct and safe. The Agda type checker doesn't give any perceivable error with
unsafe or incomplete compilation, so these tests are critical to give a
guarantee of a well done compilation. The time to create the compilation order
is negligible compare to compilation time taking at most 50 milliseconds.

% test.test_definition.TestDefinitionQueries.test_create_tree: 0.000
% .test.test_definition.TestDefinitionQueries.test_cycles: 0.622
% .test.test_definition.TestDefinitionQueries.test_dependencies: 0.000
% .test.test_definition.TestDefinitionQueries.test_dependencies_indirect: 0.000
% .test.test_definition.TestDefinitionQueries.test_dependents: 0.000
% .test.test_definition.TestDefinitionQueries.test_dependents_indirect: 0.000
% .test.test_definition.TestDefinitionQueries.test_find: 0.019
% .test.test_definition.TestDefinitionQueries.test_find_name: 0.018
% .test.test_definition.TestDefinitionQueries.test_leafs: 0.009
% .test.test_definition.TestDefinitionQueries.test_module_dependencies: 0.000
% .test.test_definition.TestDefinitionQueries.test_module_dependencies_indirect: 0.000
% .test.test_definition.TestDefinitionQueries.test_module_dependents: 0.000
% .test.test_definition.TestDefinitionQueries.test_module_dependents_indirect: 0.000
% .test.test_definition.TestDefinitionQueries.test_module_path_to_leaf: 0.009
% .test.test_definition.TestDefinitionQueries.test_nodes: 0.002
% .Node count: 53643
% test.test_definition.TestDefinitionQueries.test_nodes_count: 0.000
% .test.test_definition.TestDefinitionQueries.test_path_between: 0.000
% .test.test_definition.TestDefinitionQueries.test_path_to_leaf: 0.008
% .test.test_definition.TestDefinitionQueries.test_roots: 0.008
% .test.test_definition.TestDefinitionQueries.test_type: 0.000
% .test.test_definition.TestDefinitionQueries.test_uses: 0.015
% .test.test_definition.TestDefinitionQueries.test_uses_definition: 0.000
% .test.test_definition.TestDefinitionQueries.test_uses_indirect: 3.804
% ..........Node count: 807
% ......
% ----------------------------------------------------------------------
% Ran 38 tests in 4.517s
%
% OK

% As well as documenting system testing, this section should also describe any
% unit testing or integration testing performed. If you are not familiar with
% unit, integration or system testing then it would be a good idea to investigate
% these notions and consider about how they relate to your project. This section
% might also detail any performance, reliability or usability testing performed,
% with quantification, i.e., numeric measurements, being used wherever possible.
% All those points are systems focused through. If you’re doing something that’s
% research focused or more of a social / analytical study then you need to think
% about how you’ll measure success.

% \begin{itemize}
% \item The unit testing of agda tree definito nad module grpah, how it was done,
%   explain 
% \item The testing of the strategies in agda comp 
% \item Mention how quickly the queries run except for finding paths 
% \item Mention how long it takes to create a tree 
% \item Mention how the CLI is created directly from the query, so it is
%   unnecesary to do integration testing.
% \item Measure compilation time 
% \item Measure time to build graph 
% \item Measure time to make queries 
% \end{itemize}
