% !TEX root =  ../Dissertation.tex

\chapter{Implementation} \label{ch:implementation}

The implementation of Agda Tree must create a dependency definition graph from
an Agda project and allow the user to query this graph through a CLI. First, a
subsystem that turns Agda projects into a dependency graph needs to be
implemented. Then, the CLI that will let the user query the graph is
implemented. These subsystem must meet the functional requirements described in
\ref{tbl:Agda Tree Functional Requirements} and the non-functional requirements
described in table \ref{tbl:Agda Tree Non-Functional Requirements}. The CLI
must also implement all the queries described in table \ref{tbl:Definition Graph Queries} 
for the definition graph and the queries described in table
\ref{tbl:Module Graph Queries} for the module graph.

% This section should discuss how you went about developing a system that was
% consistent with your design to meet your stated requirements. The
% implementation of subsystems should be accurately documented, with any
% implementation difficulties being acknowledged. The Design and Implementation
% sections can be grouped in the Project Report, if these are tightly coupled.
% Likely omitted for the Project Proposal, though should mention your proposed
% implementation technologies somewhere.

\begin{itemize}
\item File structure and project structure
\end{itemize}

\section{Agda Tree}

The definition graph is the most important and difficult graph to generate. It
contains all the detailed information needed by the developer to decipher the
structure of the definitions and their relationships. Agda can already create
the module dependency graph, but there is no native feature that can create a
definition dependency graph. 

\subsection{Building the definition tree}


At first, a possible option was using the HTML files that Agda can create.
These HTML files display and format the code with colors and links. This is
vital because the HTML files would style the text differently depending if the
text was a definition, keyword, type, operator. Also, all the used definitions
were hyperlinks which connected back to the module that defined them. With a
method to find the definitions in a file given the styling and finding the
dependencies of that definition given the hyperlinks, a graph could be created.
The main issue was parsing the HTML files, finding which keywords fell into
which definition was quite difficult and using an Agda parser might have been
neccesary.

The solution is not to use the HTML files but use s-expressions. The
s-expressions are the same way the HTML files are with the same information,
except s-expressions are easier to parse. However, Agda doesn't natively
generate s-expressions but Andrej Bauer, Matej Petković, Ljupčo Todorovski in
their paper "MLFMF: Data Sets for Machine Learning for Mathematical
Formalization" \cite{bauer2023mlfmf} created an s-expression extractor. The
s-expression extractor is in the Agda backend and it will convert Agda files into
s-expressions \cite{andrej}.

\subsubsection{S-expression extractor}

S-expressions are a notation that is used in Lisp programming languges, it
represents programs and data as tree-like data structures \cite{sexp}. The grammar for the
s-expressions varies, but for this case the s-expression are of the form: (:tag
sexp-1 sexp-2 ... sexp-n). Where sexp-n can be a number, a string or another
s-expression and the tag is a keyword that describes the content of the
s-expression. The "MLFMF" paper describes in more detail
the structure of the s-expressions with respect to Agda \cite{bauer2023mlfmf}.

Here is a brief summary of the relevant s-expressions that are need for the implementation:

\begin{table}[H]
\centering
\caption{Relevant S-expressions}
\label{tbl:sexp}
\begin{tblr}{
        colspec={|X[1]|X[2]|}, hlines,
    }
sexp                              & Description                                                                                                                  \\
(:module module-name entries... ) & The root tag that holds the whole module, module-name is the name of the modules and entries are the definitions in the file \\
(:module-name name)               & The module name                                                                                                              \\
(:entries name type body)         & The definition, it includes its name, type and the body of the definition                                                    \\
(:name name)                      & The name of a definition, this name can appear as the name of an :entry tag, within the :type or :body tag                   \\
(:type type)                      & The description of the type of the parent definition                                                                         \\
(:body body)                      & The body of a definition  
\end{tblr}
\end{table}

Note that the s-expression extractor is a special version of Agda with an
extended back-end, which means the user has to compile this Agda version and
add it to their path. This will be handled by the cli described section
\ref{sub:Agda Tree CLI} to make sures the usability non-fuction requirement is
met from Table \ref{tbl:Agda Tree Non-Functional Requirements}.

\subsubsection{S-expression parser}

The :body and :type tags contain other tags like :apply, :sort, :max, etc. That
describe the definition in full detail but this information is not needed. The
information needed is the definitions in a project and what the dependencies of
those definitions which means mainly the :name and :entry tags are of interest.

\marginpar{Should I add the figure in MLFML that describes the s-expressions}

The s-expressions are compiled into a directory, this project is implemented in
Python so the library sexpdata will be used to load the raw s-expression files
into Python lists. Theses lists will be analyzed recursively, finding the
relevant tags. The strategy is to find all the :entry tags in the file, each
:entry tag represents a definition as described in Table \ref{tbl:sexp}. For
each :entry tag, find all the :name tags contained inside. With this
information create a dictionary were the key is the definition name and the
value will be a list of the :name tags. Since :entry tags describe each
definition and the :name tags describe what definition is being used, the
resulting dictionary will contain all the definitions along with their
dependencies. To find the necessary tags a \textsf{find\_exp} function is
implemented that recursively finds all occurences of a given tag within an
s-expression.

The s-expression extractor writes an s-expression file for each module, so the
mentioned dictionary is created for each file and combined together into one
big dictionary that has all the definitions from the entire project. The same
process will be repeated but instead of looking for :name tags inside the
entire :entry tag instead, find all the :name tags in the :type tag. Store this
in a dictionary with the definition as a key and the :names found in :tag as
the value. This provides the information about the type of the definition.

This parsing procedure is done in parallel, where each file is parsed in its
own thread and the dictionary of all the parsed files is combined together by
adding all the key value pairs into a bigger dictionary.

\begin{itemize}
    \item How the s-expression are extracted
    \item How the extractor is installed
    \item how the s-expressions are parsed
    \item What are s-expressions and how they work
    \item How to get the s-expression definition and relationships
    \item Explains how the tag works in this context i.e. name are definitions 
    \item How they are imported into networkx and became a tree
    \item Explain how it is stored pickled 
    \item Handling issues with recursion 
    \item Issue with the naming of the defitions 
    \item issue with where clause
\end{itemize}

\subsubsection{Building definition graph}

The dictionary with definitions as keys and their dependencies as values,
already form the definition dependency graph. Which can be imported to the
NetowrkX Python library. This library efficiently creates and queries graphs,
it contains many useful features and performance benefits. NetworkX is a widely
used tool, so a user can easily become familiar with its use and implement their
own queries. 

The graph is first initialized with the command \textsf{nx.Digraph()} then all
the definitions are added as nodes with the command
\textsf{graph.add\_nodes\_from}, where the key values of the dictionary from
the parsing is given. Lastly the edges are added with the command
\textsf{graph.add\_edges\_from} where an array is passed in containing tuples
where each tuple has the key and its dependency. Note that when creating the
edge the definition is on the first and its the depency is second, since this
is a directed graph this will cause the direction of the edge to be from the
definition to its dependency.  Once generated it will be 'pickled', a Python
library that serializes Python objects, the trees will be serialized and store
for future use. 

When parsing the s-expression files, the definitions will have its name with
its module path along with an identifier number. Due to the way the
s-expressions are extracted and the way Agda works sometimes two distinct
definitions will have the same name, so an identifier is given to distinguish
them. But this can be cumbersome for the user to deal with, so before the graph
is created the tool will attempt to remove this number unless it causes an
ambiguity.

\subsection{Building module tree} \label{sub: Building Module Tree}

Agda has a built-in feature that creates a DOT file containing all the modules
in an Agda project, including the relationship with its dependency. This
command is \textsf{agda --dependency-graph=[PATH] [Index File PATH]}. The DOT
language describes how to create nodes and edges in a Graphviz graph. This is a
standard format for graphs, NetworkX already has an extension that uses pydot,
a Python library to read, write and create DOT files. The extension can import
DOT files into NetworkX and write them back to DOT files, so once Agda
generates the DOT file it can simply be imported into NetworkX where it can be
used by the tool. This graph will then be 'pickled' and stored for future use.

\begin{itemize}
\item Explain what dot files are
\item Explain how agda extracts it
\item Explain how it is imported into networkx
\item Explain how it is stored pickled
\end{itemize}

\subsection{Queries}

Most queries are defined similarly between the definition graph and the module
graph but the module graph being acyclic means that it has different
properties. 

\marginpar{Add an example of the command and a man page describing its options}

\subsubsection{Find}

The nodes query gets all the nodes in the graph, it returns a list with all the
definition names.

\subsubsection{Find}

The find query gets all the names that match the a pattern. The user provides a
regex pattern to match on, and the query returns all the names that match.
There is a -name option, if true then it will match the pattern to the name of
the definition if it is not set then it matches on the whole name including the
modules the definition it is stored in.

\subsubsection{Dependencies}

The dependencies query gets all the dependencies of a definition. Meaning what
theorems does the definition need to be defined, either due to its type or what
it uses to be proved true. Since this is a directed dependency graph and the
definition's edges point towards its dependencies, the dependencies are the
children of the definition. NetworkX provides a method for this:
\textsf{graph.successors(definition)}.

This query must also allow for finding the indirect dependencies of a
definition, not only the direct dependencies. NetworkX provides a method for
this as well: \textsf{nx.descendants(graph, definition)}. This will find the
dependencies and the dependencies' dependencies recursively.

\subsubsection{Dependents}

The dependents query gets the dependents of a definition. The dependents would
be the theorems that use this definition either in its type or its body. In
this dependency graph, the dependants' edges point towards the definition so
the parents of a definition are its dependants. NetworkX provides a method to
get the parents and the parent's parent recursively which are
\textsf{graph.predeccessor(definition)} and \textsf{nx.ancestors(graph,
definition)} respectively.

\subsubsection{Leafs}

The leafs query gets the definitions that have no dependencies, meaning no
children. This can be found by looping through each definition and checking how
many outward edges it has with the command \textsf{graph.out\_degree(node)}, if none
they are a leaf.

\subsubsection{Module Dependencies}

This query is exclusive to the definition graph. The module dependencies query
will take the output of the dependencies query and only keep the module the
definition was in. Although, this will cause repetitions from multiple
definitions in the same module so they are added to a set to remove
repetitions.

\subsubsection{Module Dependants}

This query is exclusive to the definition graph. The module dependants query will take the output of the dependents query and
only keep the modules of dependents then added to a set to remove repetition.

\subsubsection{Path To Leaf}

The path to leaf query finds the longest path from a definition to a leaf. The
definition query is used to get the leafs of the graph, then NetworkX has a
method \textsf{nx.all\_simple\_paths(graph, definition, leafs)} which finds all
the simple paths between two nodes. Simple paths are paths where no vertex is
repeated. Once all the simple paths are found, they are measured for length and
the biggest one is returned.

The definition dependency graph isn't acyclic while the module dependency graph
is, this will cause a difference in performance of this command. The definition
graph also contains significantly more nodes that the modules graph, so the
amount of paths grows quickly.

\subsubsection{Roots}

The roots query gets the definitions that aren't used by any other theorem, it
doesn't have parents. This is found similarly to leafs but instead of checking
for outwards edges, check for inward edges with the command:
\textsf{graph.in\_degrees(node)} if it has none then its a root.

\subsubsection{Use Count}

The use count query gets the amount of times a definition is used. In other
words, how many times does a definition appear as a dependency in other
theorems. To find how many times a definition was used directly or indirectly
is the same as counting the output the dependents query. This query either
accepts a -top=n option where it will return the top n most used modules or the
-d=definition option that finds how many times a specific definition was used.

\subsubsection{Module Path To Leaf}

This query is exclusive to the definition graph. The module path to leaf query
gets the modules needed to get from one definition to another. This is done
using the path to leaf query, but only keeping the modules of the path of
definitions, repeats are removed.

\subsubsection{Definition Type}

This query is exclusive to the definition graph. The definition type query gets
the type of the definition. This data is collected during the building of the
definition graph then stored for each node.

\subsubsection{Cycles}

This query is exclusive to the definition graph. The cycles query gets the
cycles in the graph, NetworkX provides a method to find simple cycles. Simple
cycles are cycles where nodes aren't repeated, except for the start and end
node. The method is: \textsf{nx.simple\_cycles(graph)}.

\subsubsection{Save Tree}

This query is exclusive to the definition graph. The save tree query converts
the graph into the DOT format. NetworkX allows for this conversion using the
pydot library by using the method: \textsf{nx.nx\_pydot.write\_dot(graph, path) }.

\subsubsection{Path Between}

The path between query finds the longest path between to definitions. NetworkX
provides the method: \textsf{nx.all\_simple\_paths(graph, src, dst)}, were given
two nodes it will return all the simple paths between them. Simple paths are
paths that don't repeat nodes. After finding all the paths, it measures their
lengths and returns the maximum length.

\subsubsection{Level Sort}

This query is exclusive to the module graph. The level sort query sorts the
modules into levels based on how far they are from a leaf module. This is done
recursively, where the level of a node is based on the maximum level of its
children plus one.

\subsubsection{Topological Sort}

This query is exclusive to the module graph. The topological sort query sorts
the modules into a topological order. Topological sort orders the modules into
list where a module only depends on previous modules in the list.

\begin{itemize}
\item Explain how each query is implemented 
\item Explain what the english definition means for the graph 
\item Explain what the technical challenges could be 
\item Explain the algorithms that were used 
\item Explain the properties of each graph and how that limits the queries 
\item Explain the limitations of the algorithms used if any 
\item Design on how the queries are executed using argparse
\end{itemize}

\subsection{Command Line Interface}\label{sub:Agda Tree CLI}

The command line interface(CLI) will created using Python, as it is a popular
language that most users will already have installed and most users will have
some experience with. This makes it easier for users to add their own queries
and make the changes they want. At first Clojure was considered to create this
tool, as it is made to store graphs and make queries about them. However,
Clojure requires Java, isn't popular and more difficult to create queries in.
Also, NetworkX is in Python which would cause further issues. This makes Python
the best choice.

Python includes a library called argparse to create command line interfaces. In
Python the values passed in to a program are sotred ins sys.argv, argparse will
parse sys.argv based on the parsers defined. argparse will also create help and
usage messages to help the user.

There is a main parser and two sub parsers, one being for the definition
queries and the other being for the module queries. The main parser decides
whether the definition graph or module graph should be used. Then the contrl is
handled to the respective sub parsers.

The sub parsers are generated automatically from the methods that describe the
queries. There are two files def\_cmds and mod\_cmds which store the functions
that perform the queries. The functions in these file are read, and are used to
automatically generate the CLI. This allows for greater flexibility as to add
or change a query, only the function has to be changed and the interface will
update by itself.

This is done with the included Python library inspect, which can get the
functions in a file, get their parameters and their documentation. The way the
subparsers are created is by first getting all the functions names which will
be the queries inside the cmd file. For each function, the parameters it
requires will be the input the user has to give. If it is a position parameter
then the user must give it, otherwise, it is an optional parameter that can be
given by the user with -optionName. The inspect library can also read the
documentation of a function, if a comment is made below a function it is read
as documentation which is added to the help description of the query. The
documentation for each parameter has to be done manually, where a dictionary
with the parameter name and its description is used to give each option in the
CLI a description.

For example the function:

\begin{lstlisting}
def dependencies(g, d, indirect=False):
    """Definitions that definition d depends on, -indirect will find the
    indirect dependencies"""
\end{lstlisting}

becomes:

\begin{lstlisting}
agda_tree definition -h
usage: agda_tree definition [-h]
                            {dependencies}

positional arguments:
    dependencies        Definitions that definition d depends on, -indirect will find
                        the indirect dependencies
options:
  -h, --help            show this help message and exit


agda_tree definition dependencies -h

usage: agda_tree definition dependencies [-h] [-g G] [-indirect] d

positional arguments:
  d           Definition name

options:
  -h, --help  show this help message and exit
  -g G        Path to tree (Default: ~/.agda\_tree/def\_tree.pickle)
  -indirect   Get indirectly connected nodes
\end{lstlisting}

Once the parser is created, the input given by the user is evaluated and the
query can be run. To run the query the firs the appropriate graph is loaded
depending on the user's choice, this must be created before. Since the name of
the query is the same name as the function and Python allows for a function to
be called with the name of the function as a string then the name of the query
is used to find and run the function. The parameters of the function is the
same as the input parsed from the user, so this can be passed in directly to
the function. The output of the function is then printed to standard output.

It is important for the output to be printed to the console, this lets the
output be piped into other terminal applications and be used in conjunction
with other commands.

\begin{itemize}
\item Explain why using python
\item Explain what argparse is 
\item Explain how the functions are stored in a file and the methods are read into for extensibility, one responsibility principle and open close principle.
\item How the function parameters are added to the cli 
\item Explain why it is good to be a cli tool, as it can be piped and used like any other command (wz, fzf, cp) 
\item How clojure failed 
\item How cycles are difficult andc ant find distance to leafe 
\end{itemize}


Here is the help page generated for the Agda Tree definition command

\begin{lstlisting}
usage: agda_tree definition [-h]
                            {create_tree,cycles,dependencies,dependents,find,leafs,module_dependencies,module_dependents,module_path_to_leaf,nodes,path_between,path_to_leaf,roots,save_tree,type,uses} ...

positional arguments:
  {create_tree,cycles,dependencies,dependents,find,leafs,module_dependencies,module_dependents,module_path_to_leaf,nodes,path_between,path_to_leaf,roots,save_tree,type,uses}
    create_tree         Creates definition dependency tree from file, -output option to
                        set the path to store the tree
    cycles              Cycles in graph
    dependencies        Definitions that definition d depends on, -indirect will find
                        the indirect dependencies
    dependents          Definitions that depend on definition d, -indirect finds the
                        indirect dependents
    find                Find definition through regex
    leafs               Definitions with no dependencies
    module_dependencies
                        Module dependencies of definition d, -indirect finds the
                        indirect module dependencies
    module_dependents   Modules that depend on definition d, -indirect also gets the
                        indirect module dependents
    module_path_to_leaf
                        Longest path from definition d to any leaf only counting modules
    nodes               List of definitions, if -c flag is set returns the number of
                        nodes
    path_between        Longest path between two definitions src and dst
    path_to_leaf        Longest path from defintion d to any leaf
    roots               Definitions that aren't used
    save_tree           Save definition graph as pydot
    type                Types of definition d
    uses                Counts amount of uses per definition, sorted in descending
                        order, if -d is passed in a definitino it will return the uses
                        of that definition

options:
  -h, --help            show this help message and exit
\end{lstlisting}

Here is the help page generated for the Agda Tree module command
\begin{lstlisting}
usage: agda_tree module [-h]
                        {complex_modules,create_tree,dependencies,dependents,find,leafs,lvl_sort,nodes,path_between,path_to_leaf,roots,topo_sort,uses} ...

positional arguments:
  {complex_modules,create_tree,dependencies,dependents,find,leafs,lvl_sort,nodes,path_between,path_to_leaf,roots,topo_sort,uses}
    complex_modules     Get the top modules that have the most dependents
    create_tree         Creates modules dependency tree from file
    dependencies        Modules that module m imports
    dependents          Modules that import module m
    find                Find module through regex
    leafs               Modules with no imports
    lvl_sort            Level sort
    nodes               List of modules
    path_between        Longest path between two modules src and dst
    path_to_leaf        Longest path from module m to any leaf
    roots               Modules that aren't imported
    topo_sort           Topological sort
    uses                Counts how many times a module is imported, sorted in descending
                        order

options:
  -h, --help            show this help message and exit
\end{lstlisting}

\subsection{Installation} \label{sub:Agda Tree Installation}

For the tool to be distributed and easily installed by the users, it must be
packaged. The project can be packaged using a \textsf{pyproject.toml} file
which describes the metadata of the project. The Hatchling backend was chosen
to create the distribution that lets the tool be installed in different
computers. The project file also contains the dependencies needed along with
the Python version, the user can use  this file to build the project and
install it as a binary on their system.

PIP is Python's package manager, this tool can install packages from an online
repository or locally so with the project file installing this tool is as
simple as running \textsf{pip install .} . However, for end-user applications it
is better to install using PIPX which isolates the environemnt of the tool from
the remaining system, making sure that the version of a package used in the a
tool doesn't conflict with the package version in the system. PIPX can be
installed by the user with their respective package manager.


\subsection{Installing Agda S-Expression Extractor}

The Agda S-expression extractor is an extension of the Agda language by Andrej
Bauer \cite{andrej}. This extension isn't pre-built for a user to install
through their package manager or manually. To make the tool easier to install,
Agda Tree comes with a script that will download the Github repository and
compile it. 

This script clones the repository into a temporary directory and checks out the
branch to the latest version of Agda (2.7.0.1). Using Stack a tool to build
Haskell projects and manager their dependencies. Stack can be installed through
most package manager, although Agda is written is Haskell and it is likely that
Agda Developers already have this tool installed. The extractor repository
contains a YAML file that describes the project that Stack will read and build
the binaries for the extractor. The Agda binary is then coppied to the
\textsf{~/.local/bin/} folder, which the user will likely already have in their
environment path. 

One the binary is in the path, it can be accessed through the command
\textsf{agdasexp}, Agda Tree will recognize this command and use it to build
the definition graph.

\begin{itemize}
\item Explain how dependencies are handle 
\item Explain how this can be installed as a project using pip 
\item The tool automatically isntalls agdasexp 
\end{itemize}

\section{Agda Comp}

Agda Comp automatically creates the module dependency graph as described in
sub-section \ref{sub: Building Module Tree}, using Agda's built-in dependency
graph generator. The dependency graph is passed into each strategy, the
strategy will return the order in which to compile the modules. Given this
order, a 'make generator' sub-system will create the index files and the make
file to compile the modules in parallel following the order.

\subsection{Strategies}

Each strategy will take the module dependency graph as a NetworkX graph where
each node is a module and each edge is an arrow starting the at the module and
pointing towards its dependency. To compile a module, its dependencies must be
compiled first. Also, two modules can't be compiled at the same time as this
would be inefficient and could cause issues with two files being written at the
same time. A valid strategy should compile all the modules in a project and do
it safely so without compiling two modules at the same time. 

The strategies output a 3D array describing the order in which to compile the
modules and which modules can be compiled in parallel. An index is a collection
of modules that have to be compiled together, not in parallel. If two indices
have disjoint dependencies then those two indices can be compiled in parallel.
The array returned by the strategies is a list of the order in which indices
can be compiled together have to be compiled. For example, [[[module 1 , module
2] , [module 3, module 4]], [[module 5]] , [[module 6, module 7]]. What this
array shows is that "module 1" and "module 2" should be compiled together in an
index, the same for "module 3", "module 4" and "module 6", "module 7". Then
they themselves are within a list meaning that these to indices should be
compiled in parallel. So another way to write the array is the following:
[[index\_0\_0 , index\_0\_1], [index\_1\_0] , [index\_2\_0]]. Where index\_0\_0,
index\_0\_1 are compiled in parallel, the index\_1\_0 is compiled by it self then
index\_2\_0 is compiled by itself in that order.

Once the strategies have generated this array, the make generator
subsystem will create the index files that contain the modules and the make
file that will represent the order in which to compile the indices.

\subsubsection{Level Sort}

\subsubsection{Level Disjoint}

\begin{itemize}
\item Explain each strategy 
\item How it works 
\item Explain how you find modified files
\item What the motivation for it is 
\item Implication on parallelization 
\item How it was tested for safety and correctness 
\item Why using index files, to gropu these modules 
\item How the algorithm is safe and correct 
\item How make files work 
\item Why use make files over other options 
\item What the output of the algorithms is 
\item How the output is used. 
\item Difficulty with diferenct directory names and index flags
\item The limitations of each algorithm, pros and cons 
\item How they deal with multiple projects
\end{itemize}

\subsection{Creating The Make File And Indices}

An index file is a module that imports other modules but has no definitions
within it, this is done to type check all the modules in the index file,
otherwise, each imported module would have to be type checked individually.
This index files with be .lagda files, meaning the use a latex like syntax to
write the module.

Example index file from TypeTopology:

\begin{lstlisting}
    
Generated Index file

\begin{code}
    {-# OPTIONS --without-K --type-in-type --no-level-universe --no-termination-check --guardedness #-}
    import MLTT.Universes
    import MLTT.Natural-Numbers-Type
    import InfinitePigeon.Logic
    import Various.UnivalenceFromScratch
\end{code}
\end{lstlisting}

It starts with some flags that tell the type checker to change its behaviour,
for example not allowing the imports of incomplete modules. These flags are
going to be different for all projects, but the user needs to input the index
file containing all modules and the correct flags, so these options are scraped
from there. Once the flags are added to the file, a code environment is opened
where each module is imported. All the modules imported will be compiled when
that index file. The environment closes and nothing else needs to be added.


The index files are named based on the order in which they are compiled, so
index files "index-0-0" and "index-0-1" are compiled first and in parallel,
then "index-1-x" is compiled and so on. This allows for the Make file to be
created more simply. 

Example:

\begin{lstlisting}
all: _build/2.7.0.1/agda/source/index-1-2.agdai 

_build/2.7.0.1/agda/source/index-0-0.agdai: 
	agda ./source/index-0-0.lagda

_build/2.7.0.1/agda/source/index-1-0.agdai: _build/2.7.0.1/agda/source/index-0-0.agdai 
	agda ./source/index-1-0.lagda

_build/2.7.0.1/agda/source/index-1-1.agdai: _build/2.7.0.1/agda/source/index-0-0.agdai 
	agda ./source/index-1-1.lagda

_build/2.7.0.1/agda/source/index-2-0.agdai: _build/2.7.0.1/agda/source/index-1-0.agdai _build/2.7.0.1/agda/source/index-1-1.agdai _build/2.7.0.1/agda/source/index-1-2.agdai _build/2.7.0.1/agda/source/index-1-3.agdai 
	agda ./source/index-2-0.lagda
\end{lstlisting}

A makefile describes the commands that need to be run to compile a project.
Makefiles are made of targets, how to build them and what pre-requisites for
building. In the example, to compile "index-1-2.lagda" then "index-1-0.lagda"
and "index-1-1" must be compiled first. The "all:" target is the entry point to
compile the entire project meaning compiling the highest level index
"index-2-0". Agda compiles modules into .agdai files, the Makefile will check
if that .agdai file exists, if it doesn't it attempts to build it otherwise it
will continue to the next prerequisite. If the Makefile contains two targets
that don't depend on each other, it will automatically attempt to compile them
in parallel. In this case "index-1-0.lagda" is compiled in parallel with
"index-1-1.lagda".

The make file is made naturally from the output of the compilation strategy,
where the targets are the index files (named after their position in the array)
and the prerequisites are the index files that came just before it in the
array. Once all the indexes are in the Makefile, then the last index that needs
to be compiled, that depends on all the previous modules is add to the "all"
target. Note that the first index files have no dependencies, so there are no
pre-requisites.


\subsection{Command Line Interface}

The command line interface (CLI) will be created in Python, it is a popular
language that many people already have. Similarly to Agda Tree CLI Section
\ref{sub:Agda Tree CLI}, arparse is used to cream the CLI. arprase will parse
the sys.argv based on the parsers given and generate usage/help outputs.

The parser accepts the path to the all modules index file of the project to
compile as a positional parameter, then other options can be passed in to
change what strategy is used.

This is the help page for Agda Comp:
\begin{lstlisting}
usage: agda_comp [-h] [-c] [-j JOBS] [-s {level,levelb,normal,unsafe,disjoint}] module

Fast Agda type checker

positional arguments:
  module                Path to module to compile

options:
  -h, --help            show this help message and exit
  -c, --clean           Create dot file even if it already exists
  -j, --jobs JOBS       Cores that can be used
  -s, --strategy {level,levelb,normal,unsafe,disjoint}
                        The strategy that will determine the compilation order, the
                        choices are: level: Sorts modules into levels, each level
                        increses the length to a leaf, lvl 5 are the modules that 5
                        modules away from a leaf. Each level is then split into 4 files
                        or the value given by --jobs levelb: Sorts modules by levels
                        like in 'level' but instead each level is separated into n files
                        with --jobs modules each normal: Normal compilation unsafe:
                        Tries to compile all the modules with 4 concurrent index files
                        or --jobs files disjoint: Finds the biggest modules that are
                        disjoint, if none are found the leaves are removed then repeats
\end{lstlisting}

There are three options, clean, jobs and strategy. By default, the tool re-uses
the module dependency graph and removes already compiled modules from the
graph. The clean option generates the module dependency graph again and deletes
the _build directory which contains the compiled modules. Note that the way the
Agda Comp finds if a module is already compiled and hasn't changes is by
checking whether its corresponding .agdai file exists and if the the module
wasn't modified after the creation of the .agdai file. If a module was modified
after the .agdai file is created then it is considered modified. The jobs
options describe how many cores will be used in the strategy and how many cores
the make command will use. Lastly, the strategy option lets the user pick which
strategy to use.

When the user provides the index file and sets the options they desire, the
index files and make files are generated. Using the path of the index file, the
make files and indices are moved to the project root directory and the project
source directory respectively. Then the command \textsf{mk compilation.mk} is
run, where "compilation.mk" is the name of the generate make file. This will
compile the project using the cores provided by the user. Lastly, when the
compilation is done the index files and make file are deleted and the time to
compile is printed. Also, if the user chooses to cancel the compilation there
is a listener that will delete the make files and index files as to not pollute
the user's project.

For example the following command will compile TypeTopology using 2 cores, it
will delete previously compiled modules, recreate the module dependency graph
and use the levelb compilation strategy.

\begin{lstlisting}
agda_comp -j=2 -c  --strategy=levelb "/tmp/TypeTopology/source/AllModulesIndex.lagda"
\end{lstlisting}


\subsection{Installation}

The installation process for this tool is the same as for Agda Tree in Section
\ref{sub:Agda Tree Installation}. A pyproject.toml file is created describing
the dependencies, build backend and python version of the project along with
other metadata. This packages the projet such that it can be installed through
PIP, altough PIPX is recommended to install end-user applications as PIPX will
isolate the depencies of Agda Comp from the packages of the user.

\begin{itemize}
\item Explain why using python
\item Explain what argparse is 
\item Explain how the functions are stored in a file and the methods are read into for extensibility, one responsibility principle and open close principle.
\item How the function parameters are added to the cli 
\item Explain how dependencies are handle 
\item Explain how this can be installed as a project using pip 
\item Explain why it is good to be a cli tool, as it can be piped and used like any other command (wz, fzf, cp) 
\item How it is installed 
\item How the user can use it
\item How it creates the module dependency graph
\item How it uses the strategies
\item how it uses hte make and index files
\item How the modification timestamp is checked to avoid recompiling files.
\end{itemize}

\section{Conclusion}

Agda Tree will use Andrej Bauer's s-expression extractor to create the
s-expression files, which are then imported as lists into python using the
sexpdata library. These lists are explored to find all the definitions and
their relationships which are then converted into a graph in NetworkX.  Then
the CLI exposes different queries that can be made to the dependency graph to
learn more about the relationship of these definitions. The tools is packaged
such that it is easy to install and handles the building of the s-expression
extractor.

Agda Comp uses different strategies to analyze Agda's module dependency graph
to improve compilation time. The strategies read the dependency graph and
return a list that describe the order in which to compile the modules. With
this order, index files and a make file is generated that will compile the
modules in the correct order while allowing for independent index to be
compiled in parallel. The make file runs compiling the whole project, then
deletes itself along with the index file to leave the user with a clean
project. Agda Comp can use different strategies and change the amount of cores
used. The tool is easy to install with PIPX and doesn't require any external
dependencies that can't be installed through PIP.


\begin{itemize}
\item Unit testing and integration testing 
\item Documentation and version control strategies
\end{itemize}

\begin{itemize}
\item Explain the s-expressions extractor
\item How they are loaded into python 
\item How they are stored for future use 
\item How the compilation uses graph dot
\end{itemize}
