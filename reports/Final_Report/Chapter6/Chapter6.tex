% !TEX root =  ../Dissertation.tex

\chapter{Implementation} \label{ch:implementation}

The Agda Tree creates a dependency definition graph from an Agda project and
lets the user query this graph through a CLI. First, a subsystem  is made that
turns Agda projects into a dependency graph is implemented. Then, a CLI is
created that lets the user query the graph. These subsystems must meet the
functional requirements described in \cref{tbl:Agda Tree Functional
Requirements} and the non-functional requirements described in \cref{tbl:Agda
Tree Non-Functional Requirements}. The CLI implements all the queries described
in \cref{tbl:Definition Graph Queries} for the definition graph and the queries
described in \cref{tbl:Module Graph Queries} for the module graph.



% This section should discuss how you went about developing a system that was
% consistent with your design to meet your stated requirements. The
% implementation of subsystems should be accurately documented, with any
% implementation difficulties being acknowledged. The Design and Implementation
% sections can be grouped in the Project Report, if these are tightly coupled.
% Likely omitted for the Project Proposal, though should mention your proposed
% implementation technologies somewhere.

% \begin{itemize}
% \item File structure and project structure
% \end{itemize}

\section{Agda Tree}

Agda Tree uses the s-expression extractor \cite{andrej} to get the definitions
and relationships from the project. These s-expressions are parsed and
converted into a definition dependency graph. Which is then exposed through a
CLI that the user can query. Agda already includes a command to generate the
module dependency graph, so it only needs to be imported and attached to a CLI.

\subsection{Building the definition tree}

Agda's backend has the information needed to find the definitions and their
dependencies, in an Agda project. Natively there isn't a way to retrieve this
information but Andrej Bauer, Matej Petković, Ljupčo Todorovski in their paper
"MLFMF: Data Sets for Machine Learning for Mathematical Formalization"
\cite{bauer2023mlfmf} created an s-expression extractor. The s-expression
extractor works in the backend, and converts Agda source code into
s-expressions \cite{andrej} which are easy to parse.

\subsubsection{S-expression extractor}

The s-expression notation is used in Lisp programming languages, it represents
programs and data as tree-like data structures \cite{sexp}. The grammar for the
s-expressions varies, but for this case the s-expression are of the form: \texttt{(:tag
sexp-1 sexp-2 ... sexp-n)}. Where \texttt{sexp-n} can be a number, a string or another
s-expression and the tag is a keyword that describes the content of the
s-expression. The MLFMF paper describes in more detail the structure of the
s-expressions with respect to Agda \cite{bauer2023mlfmf}.

Here is a brief summary of the relevant s-expressions that are needed for the implementation:

\begin{table}[H]
\centering
\caption{Relevant S-expressions}
\label{tbl:sexp}
\begin{tblr}{
        colspec={|X[1]|X[2]|}, hlines,
    }
sexp                              & Description                                                                                                                  \\
(:module module-name entries... ) & Root tag that holds the whole module, module-name is the name of the modules and entries are the definitions in the implementation \\
(:module-name name)               & Module name                                                                                                              \\
(:entries name type body)         & Definition, it includes its name, type and the body of the definition                                                    \\
(:name name)                      & Name of a definition, this name can appear as the name of an :entry tag, within the :type or :body tag                   \\
(:type type)                      & Description of the type of the parent definition                                                                         \\
(:body body)                      & Body of a definition  
\end{tblr}
\end{table}

Note that the s-expression extractor is a modified version of Agda, which means
the user has to compile this Agda version and add it to their path. This will
be handled by the CLI described \cref{sub:Agda Tree CLI} to make sure the
usability non-functional requirement is met from \cref{tbl:Agda Tree
Non-Functional Requirements}.

\subsubsection{S-expression parser}\label{sub:s-expression parser implementation}

The \texttt{:body} and \texttt{:type} tags contain other tags like
\texttt{:apply}, \texttt{:sort}, \texttt{:max}, etc. That describe the
definition in complete detail, but this information is not needed. The
information needed is the definition names and its dependencies which means the
\texttt{:name} and \texttt{:entry} tags are of interest.

The s-expressions are compiled and saved into a directory. Then using Python
and the library sexpdata is used to load the raw s-expression files into lists.
These lists are traversed recursively to find the relevant tags. The
methodology is to find all the \texttt{:entry} tags, each \texttt{:entry} tag
represents a definition as described in \cref{tbl:sexp}. For each :entry tag,
find all the :name tags contained inside. With this information create a
dictionary where each key is the definition name and the value is a list of
:name tags. Since an \texttt{:entry} tag represents a definition and the
\texttt{:name} tags are the names of the definitions being used, the resulting
dictionary will contain all the definitions along with their dependencies. To
find the necessary tags a \texttt{find\_exp} function is implemented that
recursively traverses an s-expression, keeping track of the occurrences of a
given tag.

The s-expression extractor writes the s-expression to a file per module, so the
mentioned dictionary is created for each file and combined into one dictionary
with all the definitions. The same process repeats, instead of looking for
\texttt{:name} tags inside the \texttt{:name} tags in the \texttt{:type} tag
instead of \texttt{:entry} tag. Store this in a separate dictionary with the
definition as a key and the \texttt{:names} found in \texttt{:tag} as the
value. This provides the information about the type of the definition.

This parsing procedure is done in parallel, where each file is parsed in its
own thread and the dictionary of all the parsed files is combined by
adding all the key value pairs into a bigger dictionary.

% \begin{itemize}
%     \item How the s-expression are extracted
%     \item How the extractor is installed
%     \item how the s-expressions are parsed
%     \item What are s-expressions and how they work
%     \item How to get the s-expression definition and relationships
%     \item Explains how the tag works in this context i.e. name are definitions 
%     \item How they are imported into networkx and became a tree
%     \item Explain how it is stored pickled 
%     \item Handling issues with recursion 
%     \item Issue with the naming of the defitions 
%     \item issue with where clause
% \end{itemize}

\subsubsection{Building definition graph}

The dictionary built in \cref{sub:s-expression parser implementation} forms the
definition dependency graph, with keys as nodes and values as edges. It is
imported into the NetworkX library. This library efficiently creates and
queries graphs, it implements useful features and is performant. NetworkX is a
widely used tool, so a user can become familiar with its use and
implement their own queries. 

\todo[inline]{Do I need to explain the code this specifically} The graph is
first initialized with the command \texttt{nx.Digraph()}, a directional graph.
Then the definitions are added as nodes with the command
\texttt{graph.add\_nodes\_from}, with the keys of the dictionary passed in.
Lastly, the edges are added with the command \texttt{graph.add\_edges\_from}
where an array is passed in containing tuples with the definition and its
dependency. Note that when creating the edge the definition is the first, and
it's the dependency is second, since this is a directed graph this will cause
the direction of the edge to be from the definition to its dependency.  Once
generated it will be 'pickled', Pickle is a Python library that serializes Python
objects, the trees will be serialized and stored for future use. 

When parsing the s-expression files, the definitions names include their module
path along with an identifier number. Due to the way the s-expressions are
extracted and the way Agda works internally two distinct definitions will have
the same name with identifier to distinguish them. This can be cumbersome for
the user to deal with, so before the graph is created the tool will attempt to
remove this number unless it causes an ambiguity.

\subsection{Building module tree} \label{sub: Building Module Tree}

\todo[inline]{Do i need to mention what commands were used to create the
graph?} 

Agda has a built-in feature that creates a DOT file of the module dependency
graph of an Agda project. This command is \texttt{agda
--dependency-graph=[PATH] [Index File PATH]}. The DOT language describes how to
describe nodes and edges for a Graphviz graph. This is a standard format for
graphs, NetworkX already has an extension that uses pydot, a Python library to
read, write and create DOT files. Once Agda generates the DOT file it can be
imported into NetworkX where it can be queried. This graph will then be
pickled and stored for future use.

% \begin{itemize}
% \item Explain what dot files are
% \item Explain how agda extracts it
% \item Explain how it is imported into networkx
% \item Explain how it is stored pickled
% \end{itemize}

\subsection{Commands}

Most queries are the same between the definition graph and the module graph,
but the module graph is acyclic giving it different properties. These queries
can be explored in further detail in its repository \cite{agda_html}.

\todo[inline]{Should I trim this, or keep all the queries}
\todo[inline]{Show output of commands?}

\subsubsection{Create Tree}

The create tree command will create the dependency graph for either definitions
or modules. It will perform the parsing and extraction automatically and save
the graph to the home directory or the path of option \texttt{-o}.

\begin{lstlisting}
agda_tree definition create_tree "source/AllModulesIndex.lagda" -o=~/.agda_tree/def_tree.pickle
\end{lstlisting}

\subsubsection{Nodes}

The nodes query gets all the nodes in the graph, it returns a list with all the
definition names. For the \texttt{-c} option it will return the amount of nodes
in the graph. 

\begin{lstlisting}
agda_tree definition nodes
agda_tree definition nodes -c
\end{lstlisting}

\subsubsection{Find}

The find query gets all the names that match a pattern. The user provides a
regex pattern, and the query returns all the names that match.
There is a \texttt{-name} option, if true then it will match the pattern to the name of
the definition if it is not set then it matches on the whole name including the
modules the definition it is defined in.

\begin{lstlisting}
agda_tree definition find "\_\+\_"
agda_tree definition find "\_\+\_" -name
\end{lstlisting}

\subsubsection{Dependencies}

The dependencies query gets all the dependencies of a definition. This means
the theorems the definition needs to be implemented. Since this is a directed
dependency graph and the definition's edges point towards its dependencies, the
dependencies are the children of the definition. NetworkX provides a method for
this: \linebreak \texttt{graph.successors(definition)}.

This query must also allow for finding the indirect dependencies of a
definition, not only the direct dependencies. NetworkX provides a method for
this as well: \texttt{nx.descendants(graph, definition)}. This will find the
dependencies and their children recursively.

\begin{lstlisting}
agda_tree definition dependencies "InfinitePigeon.Addition.n-plus-zero-equals-n"
agda_tree definition dependencies -i "InfinitePigeon.Addition.n-plus-zero-equals-n"
\end{lstlisting}

\subsubsection{Dependents}

The dependents query gets the dependents of a definition. The dependents are
theorems that use this definition. In this dependency graph, the dependants'
edges point towards the definition, so the parents of a definition are its
dependants. NetworkX provides a method to get the parents and their parents
recursively which are \texttt{graph.predeccessor(definition)} and
\texttt{nx.ancestors(graph, definition)} respectively.

\begin{lstlisting}
agda_tree definition dependents "InfinitePigeon.Addition.n-plus-zero-equals-n"
agda_tree definition dependents -i "InfinitePigeon.Addition.n-plus-zero-equals-n"
\end{lstlisting}

\subsubsection{Leafs}

The leafs query gets the definitions that have no dependencies, meaning no
children. This can be found by looping through each node and counting its
outward edges with the command \texttt{graph.out\_degree(node)}, if the count
is zero they are a leaf.

\begin{lstlisting}
agda_tree definition leafs
agda_tree definition leafs -c
\end{lstlisting}

\subsubsection{Module Dependencies}

This query is exclusive to the definition graph. The module dependencies query
will take the output of the dependencies query and only keep the module the
modules of the dependencies. To avoid repeated modules from they are added to a
set. This command has an option for the indirect dependencies.

\begin{lstlisting}
agda_tree definition module_dependencies "InfinitePigeon.Addition.n-plus-zero-equals-n"
agda_tree definition module_dependencies -i "InfinitePigeon.Addition.n-plus-zero-equals-n"
\end{lstlisting}

\subsubsection{Module Dependants}

This query is exclusive to the definition graph. The module dependants query
will take the output of the dependents query and only keep the modules then add
them to a set to remove repetition. This command has an option for indirect
dependents.

\begin{lstlisting}
agda_tree definition module_dependents -i "InfinitePigeon.Addition.n-plus-zero-equals-n"
\end{lstlisting}

\subsubsection{Path To Leaf}

The path to leaf query finds the longest path from a definition to a leaf. The
leafs query is used to get the leaves of the graph, then NetworkX has a
method \texttt{nx.all\_simple\_paths(graph, definition, leaves)} which finds all
the simple paths between two nodes. Simple paths are paths where no vertex is
repeated. Once all the simple paths are found, they are measured for length and
the largest one is returned.

The definition dependency graph cyclic while the module dependency graph is
acyclic, this causes a difference in performance  The definition graph also
contains significantly more nodes than the modules graph, so the amount of
paths grows quickly.

\begin{lstlisting}
agda_tree definition path_to_leaf "InfinitePigeon.Addition.n-plus-zero-equals-n"
\end{lstlisting}

\subsubsection{Roots}

The roots query gets the definitions that aren't used by any other theorem, it
doesn't have parents. This is found similarly to leaves but instead of counting
for outward edges, count the inward edges with the method \linebreak
\texttt{graph.in\_degrees(node)} if it has none then it's a root.

\begin{lstlisting}
agda_tree definition roots
\end{lstlisting}

\subsubsection{Use Count}

The use count query gets the number of times a definition is used. In other
words, how many times does a definition appear as a dependency in other
theorems. To find how many times a definition was used directly or indirectly
is the same as counting the output of the dependents query. This query either
accepts a \texttt{-top=n} option where it will return the top n most used
modules or the \texttt{-d=definition} option that finds how many times a
specific definition was used.

\begin{lstlisting}
agda_tree definition uses -top=10
agda_tree definition uses -i -top=10
agda_tree definition uses -d "InfinitePigeon.Addition.n-plus-zero-equals-n"
agda_tree definition uses -d -i "InfinitePigeon.Addition.n-plus-zero-equals-n"
\end{lstlisting}

\subsubsection{Module Path To Leaf}

This query is exclusive to the definition graph. The module path to leaf query
gets the modules needed to get from one definition to another. This is done
using the path to leaf query, but only keeping the modules of the path of
definitions, repeats are removed.

\begin{lstlisting}
agda_tree definition module_path_to_leaf "InfinitePigeon.Addition.n-plus-zero-equals-n"
\end{lstlisting}

\subsubsection{Type}

This query is exclusive to the definition graph. The type query gets
the type of the definition. This data is collected during the building of the
definition graph then stored for each node, where it is retrieved.

\begin{lstlisting}
agda_tree definition type "InfinitePigeon.Addition.n-plus-zero-equals-n"
\end{lstlisting}

\subsubsection{Cycles}

This query is exclusive to the definition graph. The cycles query gets the
cycles in the graph, NetworkX provides a method to find simple cycles. Simple
cycles are cycles where nodes aren't repeated, except for the start and end
node. The method is: \texttt{nx.simple\_cycles(graph)}.

\begin{lstlisting}
agda_tree definition cycles
\end{lstlisting}

\subsubsection{Save Tree}

This query is exclusive to the definition graph. The save tree query converts
the graph into the DOT format. NetworkX allows for this conversion using the
pydot library by using the method: \texttt{nx.nx\_pydot.write\_dot(graph, path)}.

\begin{lstlisting}
agda_tree definition save_tree "/tmp/definition.dot"
\end{lstlisting}

\subsubsection{Path Between}

The path between query finds the longest path between to definitions. NetworkX
provides the method: \texttt{nx.all\_simple\_paths(graph, src, dst)}, were given
two nodes it will return all the simple paths between them. Simple paths are
paths that do not repeat nodes. After finding all the paths, it measures their
lengths and returns the maximum length.

\begin{lstlisting}
agda_tree definition path_between "InfinitePigeon.Addition.n-plus-zero-equals-n" "MLTT.Natural-Numbers-Type.N"
\end{lstlisting}

\subsubsection{Level Sort}

This query is exclusive to the module graph. The level sort query sorts the
modules into levels based on how far they are from a leaf module. This is done
recursively, where the level of a node is based on the maximum level of its
children plus one. This sorting is explained in \cref{sub:design level strategy}.

\begin{lstlisting}
agda_tree module lvl_sort
\end{lstlisting}

\subsubsection{Topological Sort}

This query is exclusive to the module graph. The topological sort query sorts
the modules into a topological order. Topological sort orders the modules into
list where a module only depends on previous modules in the list.


\begin{lstlisting}
agda_tree module topo_sort
\end{lstlisting}

% \begin{itemize}
% \item Explain how each query is implemented 
% \item Explain what the english definition means for the graph 
% \item Explain what the technical challenges could be 
% \item Explain the algorithms that were used 
% \item Explain the properties of each graph and how that limits the queries 
% \item Explain the limitations of the algorithms used if any 
% \item Design on how the queries are executed using argparse
% \end{itemize}

\subsection{Command Line Interface}\label{sub:Agda Tree CLI}

The CLI uses Python, as it is a popular language that
most users already have installed and may have some experience with. Making it
easier for users to add new queries and make the changes. 

Python includes a library called argparse to create CLIs. The
user inputs are stored in \texttt{sys.argv}, argparse parses it based
on the defined parsers and creates help with usage messages. There is the main
parser which contains two sub parsers, one being for the definition queries and
the other being for the module queries. The main parser decides whether the
definition graph or module graph is used. Then it is delegated to the
respective sub parsers.

\todo[inline]{Do i need to go into this much detail}
\todo[inline]{How do i format file names}

The sub parsers are generated automatically from the methods that implement the
queries. There are two files \texttt{def\_cmds.py} and \texttt{mod\_cmds.py} which store the functions
that perform the queries. The functions in this file are read, and are used to
automatically generate the CLI. This allows for greater flexibility as to add
or change a query, only the function has to be changed, and the interface will
update by itself.

This is done with the included Python library inspect, which can get the
functions in a file, get their parameters and their documentation. The way the
subparsers are generated is by first getting all the functions names which are
the queries. For each function, the parameters it requires will be the input
from the user. If it is a position parameter then the user must give it,
otherwise, it is an optional parameter. The inspect library can also read the
documentation of a function, if a comment is made below a function it is read
as documentation which is added to the help description of the query. The
documentation for each parameter has to be done manually, where a dictionary
with the parameter name and its description is used to give each option in the
CLI a description. Agda Tree's help messages can be seen in
\cref{code:definition help} and \cref{code:module help}.

For example the function:

\begin{lstlisting}
def dependencies(g, d, indirect=False):
    """Definitions that definition d depends on, -indirect will find the
    indirect dependencies"""
\end{lstlisting}

becomes:

\begin{lstlisting}
agda_tree definition dependencies -h

usage: agda_tree definition dependencies [-h] [-g G] [-indirect] d

positional arguments:
  d           Definition name

options:
  -h, --help  show this help message and exit
  -g G        Path to tree (Default: ~/.agda\_tree/def\_tree.pickle)
  -indirect   Get indirectly connected nodes
\end{lstlisting}

Once the parser is created, the user input is validated then the query
executes. To execute the query the appropriate graph is loaded, assuming the
graph to already exists. Since the name of the query is the name of the
function and Python allows for a function to be called with its name as a
string then it is used run the function. The parameters of the function are the
same as the input from the user, so this is passed in directly. The output of
the function is then printed to standard output.

It is important for the output to be printed to the console, then the
output can be piped into other terminal applications and be used in conjunction
with other commands.

% \begin{itemize}
% \item Explain why using python
% \item Explain what argparse is 
% \item Explain how the functions are stored in a file and the methods are read into for extensibility, one responsibility principle and open close principle.
% \item How the function parameters are added to the cli 
% \item Explain why it is good to be a cli tool, as it can be piped and used like any other command (wz, fzf, cp) 
% \item How clojure failed 
% \item How cycles are difficult andc ant find distance to leafe 
% \end{itemize}

\subsection{Installation} \label{sub:Agda Tree Installation}

For the tool to be distributed and installed by the users, it is packaged. The
project is packaged using the \texttt{pyproject.toml} file which describes
the metadata of the project. The Hatchling backend was chosen to create the
distribution that builds the tool for other computers. The
project file also contains the project dependencies along with the Python
version, this file is used to build the project and locally install it.

PIP is Python's package manager, it can install packages from an online
repository or locally so with the project file installing this tool is as
simple as running \texttt{pip install .} . However, for end-user applications
it is better to install using PIPX which isolates the environment of the tool
from the remaining system, to avoid clashing package version. PIPX can be
installed by the user with their respective package manager.


\subsection{Installing Agda S-Expression Extractor}

\todo[inline]{Should I explain s-expression extractor installation}

The Agda S-expression extractor is an extension of the Agda backend by Andrej
Bauer \cite{andrej}. This extension isn't pre-built for a user to download, it
must be built. To make the Agda Tree easier to install, it includes a script to
install it.

This script clones the repository into a temporary directory and checks out the
branch to the latest version of Agda (2.7.0.1). Using Stack, a tool to build
Haskell projects and manager their dependencies, to build it. Stack can be
installed through most package manager, although Agda is written in Haskell, so
Agda Developers likely already have this tool installed. The repository
contains a YAML file with instructions for Stack to build the binaries. The
binary is then copied to the \texttt{~/.local/bin/} folder, which the user
already has in their environment path. 

One the binary is in the path, it can be accessed through the command
\texttt{agdasexp}, Agda Tree will recognize this command and use it to build
the definition graph.

% \begin{itemize}
% \item Explain how dependencies are handle 
% \item Explain how this can be installed as a project using pip 
% \item The tool automatically isntalls agdasexp 
% \end{itemize}

\section{Agda Comp}

Agda Comp automatically creates the module dependency graph as described in
\cref{sub: Building Module Tree}, using Agda's built-in command. The graph is
passed into a strategy, the strategy will return the order in which to compile
the modules. Given this order, a 'make generator' sub-system creates the index
files and the make file which compile the modules in parallel following the
order.

\subsection{Strategies}

Each strategy will take the module dependency graph as a NetworkX graph where
each node is a module and each edge is an arrow starting the at the module and
pointing towards its dependency. To compile a module, its dependencies must be
compiled first. Also, two modules can't be compiled at the same time as this
would be inefficient and could cause issues with two files being written at the
same time. A valid strategy should compile all the modules in a project and do
it safely so without compiling a module and its dependency at the same time. 

The strategies output a 3D array describing the order in which to compile the
modules and which modules can be compiled in parallel. An index file is a
collection of modules that are compiled together, not in parallel. If two
indices have disjoint dependencies then those two indices can be compiled in
parallel. The array returned by the strategies is a list of the order in which
indices are compiled together. For example, \texttt{[[[module 1 , module 2] ,
[module 3, module 4]], [[module 5]] , [[module 6, module 7]]}. What this array
shows is that \texttt{module 1} and \texttt{module 2} should be compiled
together in an index, the same for \texttt{module 3}, \texttt{module 4} and
\texttt{module 6}, \texttt{module 7}. 

Then they themselves are within a list meaning that these to indices should be
compiled in parallel. So another way to write the array is the following:
\texttt{[[index\_0\_0 , index\_0\_1], [index\_1\_0] , [index\_2\_0]]}. Where
\texttt{index\_0\_0}, \texttt{index\_0\_1} are compiled in parallel, the
\texttt{index\_1\_0} is compiled by itself then \texttt{index\_2\_0} is
compiled by itself in that order. When Agda compiles a module, it has to load
in interface files that are the compiled version of the modules it depends on.
By adding as many modules into one index, means that the interface files only
have to be loaded in once limiting overhead. With this array, the make
generator subsystem will create the index files and make file that will compile
the indices in order.

\subsubsection{Level Strategy} \label{sub:imp lvl strategy}

As explained in the design \cref{sub:design level strategy}, the level
strategy works by sorting the modules into levels. Where the level of a module
is its maximum distance from a leaf, guaranteeing that if previous levels are
compiled then the modules in the current levels can be compiled in parallel.

The sorting algorithm, groups modules into levels where the level is the
maximum distance of a module to a leaf, the output is stored in a dictionary.
The level of a module is the same as the maximum level of its children plus
one, so the algorithm recursively calls the function with it's the module's
children. The dictionary stores previously analysed modules to avoid repeated
work. This algorithm is shown in \cref{code:lvl sort}

The compilation order array is created from this sorting, where each level is a
list of modules that can be compiled in parallel. Then the make file is
generated from this compilation order as explained in \cref{sub:build_make_index}.

A practical detail to note is that compiling modules individually causes a
large amount of overhead, as for Agda to type check a module it has to load
interface files of the module's dependencies which can be large. This loading
process will occur every time a module is type-checked, to avoid this overhead
the modules in the same level are grouped into index files. This way the
loading of interface files only happens once per index file.

There are two ways to split the modules in to index files, method A is to split
the modules evenly across \(n\) index files were \(n\) would be analogous to
the cores of a system. A performance increase could only happen if the computer
is doing the compilation work in parallel on separate cores, so creating more
index files than cores would cause more than 1 index file being compiled in the
same core.

Method B is to cap the amount of modules in an index file, if the cap is
exceeded another index file is created. While this will create more index files
than cores that can compile in parallel, due to the overhead of loading
interface files, being able to type check one index file while waiting for the
data of another could still result in a performance increase. These two methods
will be tested on the evaluation \cref{sub:eval comp strat} as level A
and level B.

\subsubsection{Level Disjoint Strategy} \label{sub:imp disj strategy}

As explained in \cref{sub:design disjoint strategy}, the level disjoint
strategy finds the largest disjoint modules, modules with a large amount of
dependencies which are all distinct. If the algorithm finds the disjoint
modules, it compiles them and doesn't consider them further. Otherwise, it
compiles the leafs removing common dependencies. Eventually the algorithm will
terminate as there aren't any modules left.

The challenge is in finding the disjoint modules efficiently, large projects
can have hundreds of modules, so a brute force approach isn't feasible. Thus, a
greedy approach was taken. For two modules to be disjoint from each other, that
means that the leafs they indirectly/directly depend on must also be different.
The greedy algorithm sorts the modules in ascending first by the amount leafs a
module depends on, then breaks ties by putting the module with the largest
amount of  dependencies first.

Using that sorting the greedy algorithm stores the modules into buckets. Each
bucket has modules that depend on a set of leafs. The first item of the sorted
list creates a new bucket which is added to it. Then the algorithm loops through
the remaining modules in the sorted list, if the module depends on distinct leafs
from previous buckets a new bucket is created, and it is added to it. If a
module depends on the same leafs as one of previous buckets, then it is added to it.
But if a module shares only some but not all the leafs it depends on with
another bucket, it is ignored.

The compilation order array is created by first adding the list of leafs to be
compiled as their own step, then the buckets are added as a step that can
compile each bucket in parallel. These bucket approximate the most amount
modules that can be compiled in parallel without conflict. Eventually all
modules would have been added as a leaf or a bucket to the compilation order
and the algorithm terminates.

Sometimes this algorithm will not be able to compile in parallel if only one
bucket is found, in these cases the leafs and the buckets are combined into one
compilation step.

% \begin{itemize}
% \item mention combining linear steps into one big step
% \item Explain each strategy 
% \item How it works 
% \item Explain how you find modified files
% \item What the motivation for it is 
% \item Implication on parallelization 
% \item How it was tested for safety and correctness 
% \item Why using index files, to gropu these modules 
% \item How the algorithm is safe and correct 
% \item How make files work 
% \item Why use make files over other options 
% \item What the output of the algorithms is 
% \item Difficulty with diferenct directory names and index flags
% \item The limitations of each algorithm, pros and cons 
% \item How idsjoint is np-hard, greedy algorithm to find it 
% \item There is no good way to find disjoint modules 
% \item level sort compiles onde modules at a time, meaning .agdai files are constantly loaded.
% \end{itemize}

\subsection{Building The Make File And Indices} \label{sub:build_make_index}

An index file is a module that imports other modules but has no definitions
within it, this is done to type check a collection of modules. The
\texttt{.lagda} file extension is used for the index files, meaning latex like
syntax is used to write the module. Example index file from TypeTopology:

\begin{lstlisting}
    
Generated Index file

\begin{code}
    {-# OPTIONS --without-K --type-in-type --no-level-universe --no-termination-check --guardedness #-}
    import MLTT.Universes
    import MLTT.Natural-Numbers-Type
    import InfinitePigeon.Logic
    import Various.UnivalenceFromScratch
\end{code}
\end{lstlisting}

It starts with flags that changes the behaviour of the type checker, for
example not allowing the imports of incomplete modules. These flags are going
to be different for all projects, so the flags are scraped from index file
passed in by the user. Then, code environment is opened, and each module is
imported. The environment closes and nothing else needs to be added.

The index files are named based on the order in which they are compiled, so
index files \texttt{index-0-0} and \texttt{index-0-1} are compiled first and in
parallel, then \texttt{index-1-x} is compiled next and so on. This allows for
the Make file to be generated intuitively. 

Example:

\begin{lstlisting}
all: _build/2.7.0.1/agda/source/index-1-2.agdai 

_build/2.7.0.1/agda/source/index-0-0.agdai: 
	agda ./source/index-0-0.lagda

_build/2.7.0.1/agda/source/index-1-0.agdai: _build/2.7.0.1/agda/source/index-0-0.agdai 
	agda ./source/index-1-0.lagda

_build/2.7.0.1/agda/source/index-1-1.agdai: _build/2.7.0.1/agda/source/index-0-0.agdai 
	agda ./source/index-1-1.lagda

_build/2.7.0.1/agda/source/index-2-0.agdai: _build/2.7.0.1/agda/source/index-1-0.agdai _build/2.7.0.1/agda/source/index-1-1.agdai _build/2.7.0.1/agda/source/index-1-2.agdai _build/2.7.0.1/agda/source/index-1-3.agdai 
	agda ./source/index-2-0.lagda
\end{lstlisting}

A makefile describes order of commands that need to be run to compile a
project. Makefiles are made of targets, how to build them and the prerequisites
for building them. In the example, to compile \texttt{index-1-2.lagda} then
\texttt{index-1-0.lagda} and \texttt{index-1-1.lagda} must be compiled first.
The \texttt{all:} target is the entry point to compile the entire project
meaning compiling the highest level index \texttt{index-2-0.lagda}. Agda
compiles modules into interface \texttt{.agdai} files, the Makefile will check
if interface files exist, if it doesn't, it attempts to build it otherwise it
will continue to the next prerequisite. If the Makefile contains two targets
that do not depend on each other, it will automatically attempt to compile them
in parallel. In this case \texttt{index-1-0.lagda} is compiled in parallel with
\texttt{index-1-1.lagda}.

The make file is made naturally from the output of the compilation strategy,
where the targets are the index files (named after their position in the array)
and the prerequisites are the index files that came just before it in the
array. Once all the indexes are in the Makefile, then the last index that needs
to be compiled, that depends on all the previous modules is added to the
\texttt{all:} target. Note that the first index files have no dependencies, so
there are no prerequisites.


\subsection{Command Line Interface}

The CLI is written in Python, it is a popular language
that many people already have. Similarly to Agda Tree CLI \cref{sub:Agda Tree
CLI}, argparse for the CLI. argparse parses the user input and generates
usage/help messages. The parser accepts the path to the modules file to compile
as a positional parameter, then other options can be passed in to change what
strategy to use. The help message showing these options is show in
\cref{code:agda comp help}.

There are three options, clean, jobs and strategy. By default, the tool re-uses
the previous module dependency graph and removes already compiled modules from
the graph. The clean option generates the module dependency graph again and
deletes the \texttt{\_build} directory which contains the compiled modules.
Note that Agda Comp finds if a module is already compiled and unmodified is by
checking whether its corresponding interface file exists and wasn't modified
after its creation. If a module was changed after then it is considered
modified and will be re-compiled. The jobs option sets how many cores are used
in the strategy and compilation. Lastly, the strategy option lets the user pick
which strategy to use.

When the user provides the index file and sets the options they want, the index
files and make files are generated. Using the path of the index file, the make
files and indices are moved to the project root directory and the project
source directory respectively. Then the command \texttt{mk compilation.mk} is
run, where \texttt{compilation.mk} is the name of the generated make file. This
will compile the project using the cores given by the user. Once finished the
index files and make file are deleted and the time to compile is printed. Also,
if the user chooses to cancel the compilation there is a listener that will
delete the make files and index files as to not pollute the user's project.

For example the following command will compile TypeTopology using 2 cores, it
will delete previously compiled modules, recreate the module dependency graph
and use the level B compilation strategy.

\begin{lstlisting}
agda_comp -j=2 -c  --strategy=levelb "/tmp/TypeTopology/source/AllModulesIndex.lagda"
\end{lstlisting}

\subsection{Installation}

The installation process for this tool is the same as for Agda Tree in 
\cref{sub:Agda Tree Installation}. A pyproject.toml file that describes
the dependencies, backend and python version of the project along with
other metadata. This packages the project such that it can be installed through
PIP, although PIPX is recommended to install end-user applications as PIPX will
isolate the dependencies of Agda Comp from the packages of the user.

% \begin{itemize}
% \item Explain why using python
% \item Explain what argparse is 
% \item Explain how the functions are stored in a file and the methods are read into for extensibility, one responsibility principle and open close principle.
% \item How the function parameters are added to the cli 
% \item Explain how dependencies are handle 
% \item Explain how this can be installed as a project using pip 
% \item Explain why it is good to be a cli tool, as it can be piped and used like any other command (wz, fzf, cp) 
% \item How it is installed 
% \item How the user can use it
% \item How it creates the module dependency graph
% \item How it uses the strategies
% \item how it uses hte make and index files
% \item How the modification timestamp is checked to avoid recompiling files.
% \end{itemize}

\section{Conclusion}

Agda Tree uses Andrej Bauer's s-expression extractor to create the s-expression
files, which are then imported as lists into python using the sexpdata library.
These lists are explored to find all the definitions and their relationships
which are then converted into a graph in NetworkX.  Then the CLI exposes
queries that can be made to the dependency graph for the user to learn more
about the relationship of these definitions. The tools are packaged to be easy
to install and the building of the s-expression extractor is handled.

Agda Comp uses different strategies to analyse Agda's module dependency graph
to improve compilation time. The strategies read the dependency graph and
return a list that shows the order in which to compile the modules. With this
order, index files and a make file is generated that will compile the modules
in the correct order. The make file compiles the project, then deletes itself
along with the index file to leave the user with a clean project. Agda Comp can
use different strategies and change the amount of cores used. The tool is easy
to install with PIPX and doesn't require any external dependencies that can't
be installed through PIP.


% \begin{itemize}
% \item Unit testing and integration testing 
% \item Documentation and version control strategies
% \end{itemize}

% \begin{itemize}
% \item Explain the s-expressions extractor
% \item How they are loaded into python 
% \item How they are stored for future use 
% \item How the compilation uses graph dot
% \end{itemize}

